import asyncio
from src.infrastructure.scrapers.spiders.fantasia import FantasiaSpider
from src.infrastructure.scrapers.pipeline import ScrapingPipeline
from src.infrastructure.database import SessionLocal
from src.core.logger import logger

async def run_fantasia_job():
    logger.info("ğŸš€ Starting Fantasia Job...")
    
    # 1. Scrape
    spider = FantasiaSpider()
    logger.info("ğŸ•·ï¸ Spider Initialized. Running search('auto')...")
    offers = await spider.search("auto")
    
    logger.info(f"ğŸ“¦ Collected {len(offers)} offers. Starting pipeline...")
    
    # 2. Ingest
    db = SessionLocal()
    try:
        pipeline = ScrapingPipeline(db)
        pipeline.update_database(offers)
        
        logger.info("âœ… Job Complete: Database updated.")
        # logger.info(f"ğŸ“Š Stats: Processed={stats['processed']}, Matches={stats['matches']}, Pending={stats['pending']}")
        
    except Exception as e:
        logger.error(f"âŒ Pipeline Error: {e}")
    finally:
        db.close()

if __name__ == "__main__":
    asyncio.run(run_fantasia_job())
